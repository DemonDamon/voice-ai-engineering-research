# 华为910B卡TTS并发问题关键信息

## 来源
- 华为昇腾论坛帖子
- 发布时间：2025年8月4日
- 问题状态：已解决

## 核心问题

### 1. 并发性能问题
**现状**：
- 使用CosyVoice模型在昇腾910B上直接推理，RTF无法接近1，基本在1.2且不稳定
- N卡上使用vLLM推理能满足需求
- 目标：Atlas 800T单卡实现10路并发
- 实际测试结果（基于torch npu推理）：显存足够，但RTF达不到要求

**技术限制**：
- 昇腾版本的vLLM缺少增加自定义模型的功能
- 华为侧反馈vLLM昇腾版本更新到10月份还不一定能搞定
- 建议使用Triton，但Triton昇腾版本推理CosyVoice成功案例很少
- 算法在适配Triton过程中遇到很多问题
- Triton大概率不如vLLM加速效果好

### 2. 官方建议的解决方案

#### 方案1：Profiling性能优化
- 使用Profiling进行性能拆解
- 识别模型内部存在的优化点：
  - 空泡（bubble）
  - 流同步等待
  - 调度性能等

**参考文档**：
- Profiling性能数据采集-CANN商用版8.0.RC2-昇腾社区

#### 方案2：流式输入优化
- 解读算法原理
- 问题：部分开源版本对流式输入支持不完善，导致在LLM输出文字后，需要缓存整句才能输入给TTS模块
- 优化方向：优化流式输入减少系统等待时间
- 额外等待时间计算：（单字推理时间 × 平均句字数）

#### 方案3：转OM模型方案
- 使用ATC（Ascend Tensor Compiler）将模型转换为OM格式
- 参考仓库：
  - https://gitee.com/ascend/ModelZoo-PyTorch/blob/master/ACL_PyTorch/built-in/audio/CosyVoice/CosyVoice2/README.md

## 关键技术点

### RTF性能对比
- **N卡 + vLLM**：RTF < 1（满足需求）
- **910B + Torch NPU**：RTF ≈ 1.2（不稳定）
- **目标**：单卡10路并发，RTF需要达到0.1左右

### 显存情况
- 单卡显存足够支持10路并发
- 瓶颈在计算性能而非显存

### 技术栈对比
| 技术方案 | 适配难度 | 性能预期 | 当前状态 |
|---------|---------|---------|---------|
| vLLM昇腾版 | 高（缺少自定义模型功能） | 最优 | 10月份可能支持 |
| Triton昇腾版 | 高（适配问题多） | 次优 | 成功案例少 |
| Torch NPU直推 | 低 | 差（RTF 1.2） | 已测试 |
| OM模型推理 | 中 | 待验证 | 官方推荐 |

## 客户需求
- 有偿寻求技术专家
- 可签咨询合同
- 线上咨询
- 联系邮箱：woshiqsh1986@126.com
